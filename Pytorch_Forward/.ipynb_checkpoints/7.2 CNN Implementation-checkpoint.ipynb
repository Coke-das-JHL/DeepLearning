{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accomplished-islam",
   "metadata": {},
   "source": [
    "## Implementation of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-energy",
   "metadata": {},
   "source": [
    "###  Sequenctial Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unusual-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coral-nurse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size =32\n",
    "\n",
    "N, n_C, n_H, n_W = 32,3,28,28\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "padding_size = (kernel_size-1)//2\n",
    "\n",
    "pool_size, pool_strides = 2, 2\n",
    "\n",
    "n_conv_neurons = [10,20,30]   # for 문을 통해 Conv Lavyer 끼리 Dense Layer끼리 구현하는 것도 가능\n",
    "\n",
    "n_dense_neurons = [50,30,10]\n",
    "\n",
    "x = torch.rand((N, n_C, n_H, n_W))\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Conv2d(n_C, n_conv_neurons[0],kernel_size,padding = padding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(pool_size,pool_strides ),\n",
    "            nn.Conv2d(n_conv_neurons[0], n_conv_neurons[1],kernel_size, padding = padding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(pool_size,pool_strides ),\n",
    "            nn.Conv2d(n_conv_neurons[1], n_conv_neurons[2],kernel_size, padding = padding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(pool_size,pool_strides ),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(270,n_dense_neurons[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_dense_neurons[0],n_dense_neurons[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_dense_neurons[1],n_dense_neurons[2]),\n",
    "            nn.Softmax(dim=1)\n",
    "    \n",
    ")\n",
    "\n",
    "prediction = model(x)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-cartridge",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stylish-gravity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 14, 14])\n",
      "torch.Size([32, 20, 7, 7])\n",
      "torch.Size([32, 30, 3, 3])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size =32\n",
    "\n",
    "N, n_C, n_H, n_W = 32,3,28,28\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "padding_size = (kernel_size-1)//2\n",
    "\n",
    "pool_size, pool_strides = 2, 2\n",
    "\n",
    "n_conv_neurons = [10,20,30]   # for 문을 통해 Conv Lavyer 끼리 Dense Layer끼리 구현하는 것도 가능\n",
    "\n",
    "n_dense_neurons = [50,30,10]\n",
    "\n",
    "x = torch.rand((N, n_C, n_H, n_W))\n",
    "\n",
    "class TestCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestCNN, self).__init__()\n",
    "        global n_C\n",
    "        self.conv1      = nn.Conv2d(n_C, n_conv_neurons[0],kernel_size,padding = padding_size )\n",
    "        self.conv2      = nn.Conv2d(n_conv_neurons[0], n_conv_neurons[1],kernel_size,padding = padding_size )\n",
    "        self.conv3      = nn.Conv2d(n_conv_neurons[1], n_conv_neurons[2],kernel_size,padding = padding_size )\n",
    "        self.activation = nn.ReLU()\n",
    "        self.MaxPool    = nn.MaxPool2d(pool_size,pool_strides )\n",
    "        self.flatten    = nn.Flatten()\n",
    "        self.linear1    = nn.Linear(270,n_dense_neurons[0])\n",
    "        self.linear2    = nn.Linear(n_dense_neurons[0],n_dense_neurons[1])\n",
    "        self.linear3    = nn.Linear(n_dense_neurons[1],n_dense_neurons[2])\n",
    "        self.softmax    = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.MaxPool(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.MaxPool(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.MaxPool(x)\n",
    "        print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        print(x.shape)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        print(x.shape)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "model = TestCNN()\n",
    "prediction = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-short",
   "metadata": {},
   "source": [
    "## Sequential + Layer Sub-classing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unable-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 14, 14])\n",
      "torch.Size([32, 20, 7, 7])\n",
      "torch.Size([32, 30, 3, 3])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size =32\n",
    "\n",
    "N, n_C, n_H, n_W = 32,3,28,28\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "padding_size = (kernel_size-1)//2\n",
    "\n",
    "pool_size, pool_strides = 2, 2\n",
    "\n",
    "n_chnnals = [3,10,20,30]   # for 문을 통해 Conv Lavyer 끼리 Dense Layer끼리 구현하는 것도 가능\n",
    "\n",
    "n_dense_neurons = [50,30,10]\n",
    "\n",
    "x = torch.rand((N, n_C, n_H, n_W))\n",
    "\n",
    "class MyConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(MyConv, self).__init__()\n",
    "        self.conv       = nn.Conv2d(in_channel, out_channel, kernel_size, padding = (kernel_size -1 )//2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.MaxPool    = nn.MaxPool2d(pool_size,pool_strides )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.MaxPool(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "model = nn.Sequential(\n",
    "        MyConv(n_chnnals[0],n_chnnals[1]),\n",
    "        MyConv(n_chnnals[1],n_chnnals[2]),\n",
    "        MyConv(n_chnnals[2],n_chnnals[3]),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(270,n_dense_neurons[0]),\n",
    "        nn.Linear(n_dense_neurons[0],n_dense_neurons[1]),\n",
    "        nn.Linear(n_dense_neurons[1],n_dense_neurons[2]),\n",
    "        nn.Softmax(dim=1)\n",
    ")\n",
    "prediction = model(x)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-spare",
   "metadata": {},
   "source": [
    "## Sequential + Model Sub-classing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continent-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 14, 14])\n",
      "torch.Size([32, 10, 14, 14])\n",
      "torch.Size([32, 20, 7, 7])\n",
      "torch.Size([32, 20, 7, 7])\n",
      "torch.Size([32, 30, 3, 3])\n",
      "torch.Size([32, 30, 3, 3])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 10])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size =32\n",
    "\n",
    "N, n_C, n_H, n_W = 32,3,28,28\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "padding_size = (kernel_size-1)//2\n",
    "\n",
    "pool_size, pool_strides = 2, 2\n",
    "\n",
    "n_chnnals = [3,10,20,30]   # for 문을 통해 Conv Lavyer 끼리 Dense Layer끼리 구현하는 것도 가능\n",
    "\n",
    "n_dense_neurons = [50,30,10]\n",
    "\n",
    "x = torch.rand((N, n_C, n_H, n_W))\n",
    "\n",
    "\n",
    "class MyConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(MyConv, self).__init__()\n",
    "        self.conv       = nn.Conv2d(in_channel, out_channel, kernel_size, padding = (kernel_size -1 )//2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.MaxPool    = nn.MaxPool2d(pool_size,pool_strides )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.MaxPool(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "class TestCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestCNN, self).__init__()\n",
    "        self.conv1      = MyConv(n_chnnals[0],n_chnnals[1])\n",
    "        self.conv2      = MyConv(n_chnnals[1],n_chnnals[2])\n",
    "        self.conv3      = MyConv(n_chnnals[2],n_chnnals[3])\n",
    "        self.activation = nn.ReLU()\n",
    "        self.flatten    = nn.Flatten()\n",
    "        self.linear1    = nn.Linear(270,n_dense_neurons[0])\n",
    "        self.linear2    = nn.Linear(n_dense_neurons[0],n_dense_neurons[1])\n",
    "        self.linear3    = nn.Linear(n_dense_neurons[1],n_dense_neurons[2])\n",
    "        self.softmax    = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        print(x.shape)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        print(x.shape)\n",
    "        x = self.linear3(x)\n",
    "        x = self.softmax(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "model = TestCNN()\n",
    "prediction = model(x)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collective-economy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 14, 14])\n",
      "torch.Size([32, 20, 7, 7])\n",
      "torch.Size([32, 30, 3, 3])\n",
      "torch.Size([32, 30, 3, 3])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size =32\n",
    "\n",
    "N, n_C, n_H, n_W = 32,3,28,28\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "padding_size = (kernel_size-1)//2\n",
    "\n",
    "pool_size, pool_strides = 2, 2\n",
    "\n",
    "n_chnnals = [3,10,20,30]   # for 문을 통해 Conv Lavyer 끼리 Dense Layer끼리 구현하는 것도 가능\n",
    "\n",
    "n_dense_neurons = [50,30,10]\n",
    "\n",
    "x = torch.rand((N, n_C, n_H, n_W))\n",
    "\n",
    "\n",
    "class MyConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(MyConv, self).__init__()\n",
    "        self.conv       = nn.Conv2d(in_channel, out_channel, kernel_size, padding = (kernel_size -1 )//2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.MaxPool    = nn.MaxPool2d(pool_size,pool_strides )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.MaxPool(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "class TestCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestCNN, self).__init__()\n",
    "        self.fe        =nn.Sequential()\n",
    "        self.fe.add_module(\"conv_1\",MyConv(n_chnnals[0],n_chnnals[1]))\n",
    "        self.fe.add_module(\"conv_2\",MyConv(n_chnnals[1],n_chnnals[2]))\n",
    "        self.fe.add_module(\"conv_3\",MyConv(n_chnnals[2],n_chnnals[3]))\n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('flatten',nn.Flatten() )\n",
    "        self.classifier.add_module('Dense1' ,nn.Linear(270,n_dense_neurons[0]) )\n",
    "        self.classifier.add_module('relu'   ,nn.ReLU() )\n",
    "        self.classifier.add_module('Dense2' ,nn.Linear(n_dense_neurons[0],n_dense_neurons[1]) )\n",
    "        self.classifier.add_module('Dense3' ,nn.Linear(n_dense_neurons[1],n_dense_neurons[2]) )\n",
    "        self.classifier.add_module('softmax',nn.Softmax(dim=1) )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fe(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "model = TestCNN()\n",
    "prediction = model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
