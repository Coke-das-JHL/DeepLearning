{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nearby-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "filled-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, n_H, n_W, n_C = 1, 28, 28,3\n",
    "n_filter = 2\n",
    "k_size   = 3\n",
    "\n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abstract-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([2, 3, 3, 3])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 2, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ... )\n",
    "conv = nn.Conv2d(n_C,n_filter,k_size )\n",
    "\n",
    "y = conv(images)\n",
    "\n",
    "weight, bias = conv.weight, conv.bias\n",
    "\n",
    "# Torch's image tensor (N, C, H, W) \n",
    "print(images.shape)\n",
    "print(weight.shape)\n",
    "print(bias.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-surge",
   "metadata": {},
   "source": [
    "## Correlation with Single Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "owned-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(torch) : \n",
      " tensor([[-0.6951, -0.6117, -0.6043],\n",
      "        [-0.6553, -0.4003, -0.8373],\n",
      "        [-0.6819, -0.7940, -0.6573]], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "N, n_H, n_W, n_C = 1, 5, 5, 1\n",
    "n_filter = 1\n",
    "k_size   = 3\n",
    "\n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))\n",
    "\n",
    "conv = nn.Conv2d(n_C,n_filter,k_size )\n",
    "\n",
    "y = conv(images)\n",
    "print(\"Y(torch) : \\n\", y.squeeze())  # squeeze() : 차원이 1인 축을 축소시킴\n",
    "weight, bias = conv.weight, conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "veterinary-bridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(3, 3)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "images = images.detach().numpy().squeeze()\n",
    "weight = weight.detach().numpy().squeeze()\n",
    "bias = bias.detach().numpy()\n",
    "print(images.shape)\n",
    "print(weight.shape)\n",
    "print(bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "related-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros((n_H-k_size+1, n_W-k_size+1))\n",
    "\n",
    "for i in range(n_H-k_size+1):\n",
    "    for j in range(n_W-k_size+1):\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contrary-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(numpy) : \n",
      " [[-0.6951 -0.6117 -0.6043]\n",
      " [-0.6553 -0.4003 -0.8373]\n",
      " [-0.6819 -0.794  -0.6573]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_H-k_size+1):\n",
    "    for j in range(n_W-k_size+1):\n",
    "        window = images[i:i+k_size,j:j+k_size]\n",
    "        y[i,j] = np.sum(window*weight) + bias\n",
    "\n",
    "print(\"Y(numpy) : \\n\", np.round(y,4))  # squeeze() : 차원이 1인 축을 축소시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-brisbane",
   "metadata": {},
   "source": [
    "## Correlation with Multiple Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "purple-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(torch) : \n",
      " tensor([[-0.2658, -0.5584, -0.3178],\n",
      "        [-0.6282, -0.4146, -0.4887],\n",
      "        [-0.4071, -0.7173, -0.3022]], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "N, n_H, n_W, n_C = 1, 5, 5, 3\n",
    "n_filter = 1\n",
    "k_size   = 3\n",
    "\n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))\n",
    "\n",
    "conv = nn.Conv2d(n_C,n_filter,k_size )\n",
    "\n",
    "y = conv(images)\n",
    "print(\"Y(torch) : \\n\", y.squeeze())  # squeeze() : 차원이 1인 축을 축소시킴\n",
    "weight, bias = conv.weight, conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "superb-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 5)\n",
      "(3, 3, 3)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "images = images.detach().numpy().squeeze()\n",
    "weight = weight.detach().numpy().squeeze()\n",
    "bias = bias.detach().numpy()\n",
    "print(images.shape)\n",
    "print(weight.shape)\n",
    "print(bias.shape)      # Channel Order Check!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coastal-empire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y(numpy) : \n",
      " [[-0.2658 -0.5584 -0.3178]\n",
      " [-0.6282 -0.4146 -0.4887]\n",
      " [-0.4071 -0.7173 -0.3022]]\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros((n_H-k_size+1, n_W-k_size+1))\n",
    "\n",
    "for i in range(n_H-k_size+1):\n",
    "    for j in range(n_W-k_size+1):\n",
    "        window = images[:,i:i+k_size,j:j+k_size] \n",
    "        y[i,j] = np.sum(window*weight) + bias\n",
    "\n",
    "print(\"Y(numpy) : \\n\", np.round(y,4))  # squeeze() : 차원이 1인 축을 축소시킴"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
