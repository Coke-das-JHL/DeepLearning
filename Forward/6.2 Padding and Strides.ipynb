{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qualified-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-lunch",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "applicable-sister",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3])\n",
      "torch.Size([1, 3, 5, 5])\n",
      "======================================================================\n",
      "tensor([[[[0.4863, 0.4628, 0.0628],\n",
      "          [0.2033, 0.1343, 0.3362],\n",
      "          [0.5537, 0.7599, 0.2339]],\n",
      "\n",
      "         [[0.4736, 0.3247, 0.5010],\n",
      "          [0.1171, 0.0245, 0.1676],\n",
      "          [0.6107, 0.8692, 0.6566]],\n",
      "\n",
      "         [[0.1209, 0.1579, 0.8028],\n",
      "          [0.8313, 0.7043, 0.0209],\n",
      "          [0.9005, 0.7911, 0.1204]]]])\n",
      "======================================================================\n",
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4863, 0.4628, 0.0628, 0.0000],\n",
      "          [0.0000, 0.2033, 0.1343, 0.3362, 0.0000],\n",
      "          [0.0000, 0.5537, 0.7599, 0.2339, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.4736, 0.3247, 0.5010, 0.0000],\n",
      "          [0.0000, 0.1171, 0.0245, 0.1676, 0.0000],\n",
      "          [0.0000, 0.6107, 0.8692, 0.6566, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.1209, 0.1579, 0.8028, 0.0000],\n",
      "          [0.0000, 0.8313, 0.7043, 0.0209, 0.0000],\n",
      "          [0.0000, 0.9005, 0.7911, 0.1204, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.ZeroPad2d(padding)\n",
    "\n",
    "N, n_H, n_W, n_C = 1, 3, 3,3\n",
    "\n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))\n",
    "\n",
    "pad = nn.ZeroPad2d(1)\n",
    "padded_img = pad(images)\n",
    "\n",
    "print(images.shape)\n",
    "print(padded_img.shape)\n",
    "print('='*70)\n",
    "print(images)\n",
    "print('='*70)\n",
    "print(padded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "N, n_H, n_W, n_C = 1, 3, 3,3\n",
    "kernel_size = 3 \n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))\n",
    "\n",
    "\n",
    "conv = nn.Conv2d(3,1,kernel_size,padding=1, stride=1)\n",
    "\n",
    "y = conv(images)\n",
    "\n",
    "print(images.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-costume",
   "metadata": {},
   "source": [
    "## Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantitative-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "N, n_H, n_W, n_C = 1, 28, 28,3\n",
    "kernel_size = 3 \n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))\n",
    "\n",
    "\n",
    "conv = nn.Conv2d(3,1,kernel_size,padding=0, stride=2)\n",
    "\n",
    "y = conv(images)\n",
    "\n",
    "print(images.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "statutory-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([1, 3, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "N, n_H, n_W, n_C = 1, 28, 28,3\n",
    "kernel_size = 3 \n",
    "# Torch's image tensor (N, C, H, W) \n",
    "images = torch.rand((N,n_C, n_H, n_W ))\n",
    "\n",
    "\n",
    "conv = nn.MaxPool2d(kernel_size,stride=2)\n",
    "\n",
    "y = conv(images)\n",
    "\n",
    "print(images.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-store",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
