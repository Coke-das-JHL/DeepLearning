{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "protective-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-groove",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "encouraging-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Logits \n",
      " tensor([[-0.4072,  0.7402,  2.5964,  3.8201, -0.7910],\n",
      "        [ 0.2692,  0.7671, -1.7903, -1.4712,  4.1446]]) \n",
      "\n",
      "Softmax Output \n",
      " tensor([[0.0107, 0.0337, 0.2156, 0.7328, 0.0073],\n",
      "        [0.0195, 0.0322, 0.0025, 0.0034, 0.9424]]) \n",
      "\n",
      "Sum of Softmax Output \n",
      "  tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "logits = 10 * ( torch.rand((2,5)) - 0.5 )\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "p_vector = softmax(logits)  \n",
    "\n",
    "print('Input Logits \\n', logits,'\\n')\n",
    "print('Softmax Output \\n', p_vector,'\\n')\n",
    "print('Sum of Softmax Output \\n ', torch.sum(p_vector,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-digit",
   "metadata": {},
   "source": [
    "# Softmax in Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "parallel-retail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7258e-02, 1.3010e-01, 6.8685e-01, 1.2988e-01, 2.2537e-02, 1.3371e-02],\n",
      "        [5.0495e-01, 3.9171e-02, 4.2723e-03, 3.1348e-01, 3.3819e-02, 1.0430e-01],\n",
      "        [7.5292e-02, 1.8671e-01, 1.3505e-01, 2.2297e-02, 4.6675e-01, 1.1390e-01],\n",
      "        [2.8586e-01, 8.3367e-02, 2.4353e-01, 1.4939e-01, 7.3324e-02, 1.6452e-01],\n",
      "        [1.6620e-01, 5.3034e-03, 3.7374e-02, 7.3499e-01, 2.8760e-04, 5.5846e-02],\n",
      "        [1.6161e-02, 4.3445e-02, 6.5368e-01, 1.2507e-03, 1.1440e-01, 1.7106e-01],\n",
      "        [2.0685e-01, 2.4858e-01, 1.8740e-01, 5.2040e-02, 8.5754e-02, 2.1937e-01],\n",
      "        [7.1589e-02, 2.4863e-01, 6.6761e-02, 6.6869e-03, 4.2098e-01, 1.8536e-01]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "n_data = 8\n",
    "n_feature = 5 \n",
    "n_class = 6\n",
    "\n",
    "logits = 10 * ( torch.rand((n_data,n_feature)) - 0.5 )\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(n_feature,n_class),\n",
    "            nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "output = model(logits)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-torture",
   "metadata": {},
   "source": [
    "# Multiclass  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cosmetic-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([8, 5])\n",
      "Layer1 Output shape :  torch.Size([8, 8])\n",
      "Layer2 Output shape :  torch.Size([8, 5])\n",
      "output shape :  torch.Size([8, 3])\n",
      "tensor([[0.3742, 0.4250, 0.2008],\n",
      "        [0.3799, 0.3535, 0.2665],\n",
      "        [0.3671, 0.3704, 0.2625],\n",
      "        [0.3262, 0.3512, 0.3225],\n",
      "        [0.3640, 0.4318, 0.2041],\n",
      "        [0.3669, 0.4095, 0.2236],\n",
      "        [0.3731, 0.4059, 0.2210],\n",
      "        [0.3071, 0.3439, 0.3490]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class testModel(nn.Module):\n",
    "    def __init__(self,i_feature,o_feature):\n",
    "        super(testModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_feature, 8)\n",
    "        self.fc2 = nn.Linear(8, 5)\n",
    "        self.fc3 = nn.Linear(5, o_feature )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('input shape : ', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        print('Layer1 Output shape : ', x.shape)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        print('Layer2 Output shape : ', x.shape)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        print('output shape : ', x.shape)\n",
    "        return x\n",
    "    \n",
    "n_feature = 5\n",
    "n_class   = 3\n",
    "logits = 10 * ( torch.rand((8,n_feature)) - 0.5 )\n",
    "\n",
    "model = testModel(n_feature, n_class)\n",
    "y = model.forward(logits)\n",
    "print(y)\n",
    "print()\n",
    "print(torch.sum(y, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
